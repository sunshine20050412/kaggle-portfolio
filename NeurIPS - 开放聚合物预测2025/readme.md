# ğŸ§ª NeurIPS â€” Open Polymer Prediction 2025

> **åŸºäºå¤šè§†è§’è¡¨å¾ä¸ GNN/CatBoost/XGBoost ä¸‰è·¯èåˆçš„èšåˆç‰©ç‰©æ€§é¢„æµ‹**
>
> **Predicting five polymer properties via multi-view molecular representations and a three-route GNN / CatBoost / XGBoost ensemble**
>
> Kaggle Competition Â· 2025

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red.svg)](https://pytorch.org)
[![PyG](https://img.shields.io/badge/PyTorch_Geometric-2.6.1-orange.svg)](https://pyg.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Kaggle](https://img.shields.io/badge/Kaggle-Competition-blue.svg)](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025)

**ğŸŒ Language / è¯­è¨€ï¼šæœ¬æ–‡æ¡£ä¸ºä¸­è‹±æ–‡å¯¹ç…§ç‰ˆ â€” This document is bilingual (Chinese & English)**

---

## ğŸ“Œ é¡¹ç›®æ¦‚è¿° / Project Summary

### æ ¸å¿ƒæ€è·¯ / Core Idea

ä¸æŒ‡æœ›å•ä¸ªæ¨¡å‹ç‹¬ç«‹å®Œæˆèšåˆç‰©ç‰©æ€§é¢„æµ‹ï¼Œè€Œæ˜¯åˆ†ä¸‰æ¡ç‹¬ç«‹è·¯å¾„â€”â€”åˆ†å­å›¾ GNNã€Mordred æè¿°ç¬¦ + CatBoostã€Morgan/MACCS æŒ‡çº¹ + å›¾ç»Ÿè®¡ + XGBoostâ€”â€”åˆ†åˆ«ä»ä¸åŒç²’åº¦ç†è§£åˆ†å­ç»“æ„ï¼Œæœ€åé€šè¿‡çº¿æ€§åŠ æƒèåˆäº’è¡¥ä¿¡æ¯ã€‚æ•°æ®æ²»ç†ï¼ˆR-group è¿‡æ»¤ + canonical SMILES æ ‡å‡†åŒ–ï¼‰å’Œå¤šæºå¤–éƒ¨æ•°æ®çš„å¹¶å…¥ï¼Œæ˜¯æ•´ä¸ªæ–¹æ¡ˆç¨³å®šæ€§çš„åŸºç¡€ã€‚

Rather than relying on a single model, the solution builds three independent routes â€” molecular-graph GNN, Mordred descriptors + CatBoost, and Morgan/MACCS fingerprints + graph statistics + XGBoost â€” each capturing molecular structure at a different granularity. The predictions are fused via learned linear weights. Robust data curation (R-group filtering, canonical SMILES normalization) and multi-source external data integration form the stability foundation of the entire pipeline.

### ä¸»è¦æ”¶è· / Key Takeaways

- **æ•°æ®æ²»ç†å…ˆäºå»ºæ¨¡** â€” R-group è¿‡æ»¤ä¸ canonical SMILES æ ‡å‡†åŒ–æ˜¯é¿å…é™é»˜é”™è¯¯çš„ç¬¬ä¸€é“é˜²çº¿ï¼›å¤–éƒ¨æ•°æ®çš„å¹¶å…¥å¯¹æ ·æœ¬ç¨€ç¼ºçš„ç›®æ ‡ï¼ˆTcã€FFVï¼‰å½±å“å°¤å¤§ã€‚
- **Data curation before modeling** â€” R-group filtering and canonical SMILES normalization are the first line of defense against silent errors; external data integration particularly helps targets with sparse labels (Tc, FFV).
- **å¤šè§†è§’ä¼˜äºå•è§†è§’** â€” GNN æ•æ‰æ‹“æ‰‘ç»“æ„ï¼Œæ ‘æ¨¡å‹åˆ©ç”¨é«˜ç»´æè¿°ç¬¦ï¼Œä¸‰è€…èåˆçš„è¡¨ç°ä¼˜äºä»»ä½•å•è·¯æ¨¡å‹ã€‚
- **Multi-view beats single-view** â€” GNN captures topology; tree models exploit high-dimensional descriptors. The three-route ensemble outperforms any individual route.
- **ç›®æ ‡ç‹¬ç«‹å»ºæ¨¡ > å¤šä»»åŠ¡å­¦ä¹ ** â€” äº”ä¸ªç›®æ ‡çš„æ•°æ®é‡å’Œæ•°æ®æºå„ä¸ç›¸åŒï¼Œç‹¬ç«‹å»ºæ¨¡èƒ½ä¸ºæ¯ä¸ªç›®æ ‡é€‰æ‹©æœ€ä¼˜çš„è®­ç»ƒå­é›†å’Œè¶…å‚æ•°ã€‚
- **Per-target modeling > multi-task** â€” Each target has different sample sizes and data sources; independent modeling lets each target use its own optimal subset and hyperparameters.
- **ç‰¹å¾ç­›é€‰çš„ç›®æ ‡ç‰¹å¼‚æ€§** â€” XGBoost è·¯çº¿ä¸­å¯¹æ¯ä¸ªç›®æ ‡ç»´æŠ¤ä¸€ä»½ç‹¬ç«‹çš„ç‰¹å¾ç™½åå•ï¼ŒåŠ ä¸Šæ–¹å·®é˜ˆå€¼è£å‰ªï¼Œæœ‰æ•ˆé™ä½äº†å™ªå£°ç‰¹å¾å¯¹æ ‘æ¨¡å‹çš„å¹²æ‰°ã€‚
- **Target-specific feature selection** â€” Maintaining per-target feature whitelists plus variance-threshold pruning in the XGBoost route effectively reduces noise feature interference on tree models.

---

## ğŸ“‹ ç›®å½• / Table of Contents

- [é¡¹ç›®æ¦‚è¿° / Project Summary](#-é¡¹ç›®æ¦‚è¿°--project-summary)
- [èµ›é¢˜èƒŒæ™¯ / Competition Background](#-èµ›é¢˜èƒŒæ™¯--competition-background)
- [æ–¹æ¡ˆæ¦‚è§ˆ / Solution Overview](#-æ–¹æ¡ˆæ¦‚è§ˆ--solution-overview)
- [æ•°æ®æ²»ç†ä¸å¤–éƒ¨æ•°æ® / Data Curation & External Data](#-æ•°æ®æ²»ç†ä¸å¤–éƒ¨æ•°æ®--data-curation--external-data)
- [ä¸‰è§†è§’å»ºæ¨¡ / Three-Route Modeling](#-ä¸‰è§†è§’å»ºæ¨¡--three-route-modeling)
- [æ¨¡å‹èåˆ / Model Fusion](#-æ¨¡å‹èåˆ--model-fusion)
- [å…³é”®å·¥ç¨‹å†³ç­– / Key Engineering Decisions](#-å…³é”®å·¥ç¨‹å†³ç­–--key-engineering-decisions)
- [ä»“åº“ç»“æ„ / Repository Structure](#-ä»“åº“ç»“æ„--repository-structure)
- [å¤ç°æŒ‡å— / Reproduction Guide](#-å¤ç°æŒ‡å—--reproduction-guide)
- [åæ€ä¸æœªæ¥æ–¹å‘ / Reflections & Future Directions](#-åæ€ä¸æœªæ¥æ–¹å‘--reflections--future-directions)

---

## ğŸŸ èµ›é¢˜èƒŒæ™¯ / Competition Background

**ä¸­æ–‡ï¼š**
NeurIPS 2025 Open Polymer Prediction æ˜¯ä¸€åœºå¤šç›®æ ‡å›å½’ç«èµ›ã€‚ç»™å®šèšåˆç‰©çš„ SMILES è¡¨ç¤ºï¼Œè¦æ±‚ç›´æ¥é¢„æµ‹äº”é¡¹ç‰©æ€§ï¼šç»ç’ƒè½¬ç§»æ¸©åº¦ï¼ˆTgï¼‰ã€è‡ªç”±ä½“ç§¯åˆ†æ•°ï¼ˆFFVï¼‰ã€ä¸´ç•Œæ¸©åº¦ï¼ˆTcï¼‰ã€å¯†åº¦ï¼ˆDensityï¼‰å’Œå›è½¬åŠå¾„ï¼ˆRgï¼‰ã€‚æ ‡ç­¾æ¥æºäºå¤šæ¬¡åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹¼çš„å‡å€¼ï¼Œéšè—æµ‹è¯•é›†è§„æ¨¡çº¦ 1.5Kã€‚è¯„ä¼°æŒ‡æ ‡ä¸º wMAEï¼ˆæŒ‰æ ·æœ¬ç¨€ç¼ºåº¦ä¸å–å€¼èŒƒå›´é‡åŠ æƒï¼‰ï¼Œç¡®ä¿å„ç›®æ ‡åŒç­‰é‡è¦ã€‚æ¯”èµ›é™åˆ¶ä»£ç è¿è¡Œåœ¨ Kaggle Notebook å†…ï¼Œç¦æ­¢è”ç½‘ï¼Œå•æ¬¡ CPU/GPU è¿è¡Œæ—¶é—´ä¸è¶…è¿‡ 9 å°æ—¶ã€‚

**English:**
NeurIPS 2025 Open Polymer Prediction is a multi-target regression competition. Given polymer SMILES, contestants predict five physical properties: glass-transition temperature (Tg), fractional free volume (FFV), critical temperature (Tc), density, and radius of gyration (Rg). Labels are averages from multiple molecular-dynamics simulations; the hidden test set is ~1.5K samples. The metric is wMAE â€” weighted by sample scarcity and value range â€” to ensure equal importance across targets. Code must run within a Kaggle Notebook (no internet, â‰¤ 9 h GPU time).

**ä¸»è¦æŒ‘æˆ˜ / Key Challengesï¼š**

| æŒ‘æˆ˜ / Challenge | è¯´æ˜ / Description |
|---|---|
| å¤šç›®æ ‡å¼‚æ„ / Multi-target heterogeneity | äº”ä¸ªç›®æ ‡çš„æ ·æœ¬é‡å·®å¼‚æå¤§ï¼Œéƒ¨åˆ†ç›®æ ‡æ•°æ®æç¨€ç¼º / Huge sample-size gaps; some targets are very sparse |
| SMILES å™ªå£° / SMILES noise | åŸå§‹æ•°æ®å« R-group è®°å·å’Œéæ ‡å‡†å†™æ³•ï¼ŒåŒæ„åˆ†å­å¯æœ‰å¤šç§ SMILES / Raw data contains R-group notation and non-canonical forms |
| è¯„ä¼°æŒ‡æ ‡å¤æ‚ / Complex metric | wMAE çš„é‡åŠ æƒæœºåˆ¶ä½¿å¾—ç®€å•ä¼˜åŒ– MAE å¹¶ä¸ç­‰ä»·äºä¼˜åŒ–æœ€ç»ˆæŒ‡æ ‡ / wMAE re-weighting means naively optimizing MAE â‰  optimizing the final metric |
| ç¯å¢ƒçº¦æŸ / Environment constraints | Kaggle Notebook ç¦ç½‘ã€æ—¶é—´é™åˆ¶ï¼Œéœ€ç¦»çº¿å®‰è£…å…¨éƒ¨ä¾èµ– / No internet; all dependencies must be installed offline |

---

## ğŸŒŸ æ–¹æ¡ˆæ¦‚è§ˆ / Solution Overview

```
æ ¸å¿ƒæ´å¯Ÿ / Key Insight:
  åŒä¸€ä¸ªåˆ†å­ï¼Œç”¨å›¾çœ‹ã€ç”¨æè¿°ç¬¦çœ‹ã€ç”¨æŒ‡çº¹çœ‹ï¼Œå¾—åˆ°çš„ä¿¡æ¯äº’è¡¥ã€‚
  ä¸‰æ¡è·¯ç‹¬ç«‹èµ°ï¼Œæœ€ååŠ æƒåˆå¹¶ï¼Œæ¯”ä»»ä½•å•è·¯éƒ½ç¨³ã€‚
  The same molecule looks different as a graph, as descriptors, as fingerprints.
  Three independent routes, fused at the end, beat any single route.
```

| ç»„ä»¶ / Component | æ–¹æ³• / Approach |
|---|---|
| **æ•°æ®æ²»ç† / Data Curation** | R-group è¿‡æ»¤ â†’ RDKit è§£ææ ¡éªŒ â†’ canonical SMILES æ ‡å‡†åŒ– / R-group filter â†’ RDKit parse validation â†’ canonical SMILES |
| **å¤–éƒ¨æ•°æ® / External Data** | å¤šæº Tg / Tc / Density / FFV æ•°æ®ï¼Œä»¥ canonical SMILES ä¸ºé”®å»é‡å¹¶å‡å€¼èšåˆ / Multi-source data, keyed & deduplicated by canonical SMILES |
| **è·¯çº¿ A / Route A** | åˆ†å­å›¾ â†’ GCN + GAT â†’ mean/max æ± åŒ– + åˆ†å­çº§ç‰¹å¾ â†’ ç‹¬ç«‹å›å½’å¤´ / Mol graph â†’ GCN + GAT â†’ pool + mol features â†’ per-target heads |
| **è·¯çº¿ B / Route B** | Mordred 2D æè¿°ç¬¦ â†’ CatBoost å›å½’ï¼ˆç›®æ ‡ç‹¬ç«‹ï¼‰ / Mordred 2D descriptors â†’ CatBoost (per-target) |
| **è·¯çº¿ C / Route C** | Morgan(r=2, 128bit) + MACCS(166bit) + RDKit ç‰©åŒ–æè¿°ç¬¦ + NetworkX å›¾ç»Ÿè®¡ â†’ ç›®æ ‡ç‰¹å¼‚æ€§ç‰¹å¾ç­›é€‰ â†’ XGBoost / Fingerprints + descriptors + graph stats â†’ per-target feature selection â†’ XGBoost |
| **èåˆ / Fusion** | çº¿æ€§åŠ æƒï¼šGNN 0.4 / CatBoost 0.3 / XGBoost 0.3ï¼Œäº”ä¸ªç›®æ ‡ç‹¬ç«‹åŠ æƒ / Linear blend: GNN 0.4 / Cat 0.3 / XGB 0.3, per-target independent |

### ç®—æ³•æµç¨‹å›¾ / Pipeline Diagram

```mermaid
flowchart TD
    A["ğŸ“‚ åŸå§‹ SMILES\nRaw Polymer SMILES"]

    A --> B["ğŸ§¹ æ•°æ®æ²»ç†\nR-groupè¿‡æ»¤ Â· canonicalåŒ– Â· RDKitæ ¡éªŒ\nR-group filter Â· canonicalize Â· RDKit validate"]
    B --> C["ğŸ“¦ å¤–éƒ¨æ•°æ®å¹¶å…¥\nTg / Tc / Density / FFV å¤šæº\nMulti-source external data"]
    C --> D["è®­ç»ƒé›†æ‰©å±•\nExtended Training Set"]

    D --> E["è·¯çº¿A / Route A\nåˆ†å­å›¾ GNN"]
    D --> F["è·¯çº¿B / Route B\nMordred + CatBoost"]
    D --> G["è·¯çº¿C / Route C\næŒ‡çº¹+å›¾ç»Ÿè®¡ + XGBoost"]

    E --> H["gnn_sub.csv"]
    F --> I["cat_sub.csv"]
    G --> J["xgb_sub.csv"]

    H --> K["âš–ï¸ åŠ æƒèåˆ\nw_GNN=0.4 Â· w_Cat=0.3 Â· w_XGB=0.3"]
    I --> K
    J --> K

    K --> L["ğŸ“„ submission.csv"]
```

---

## ğŸ§¹ æ•°æ®æ²»ç†ä¸å¤–éƒ¨æ•°æ® / Data Curation & External Data

### SMILES æ¸…æ´—æµç¨‹ / SMILES Cleaning Pipeline

èšåˆç‰© SMILES ä¸­å¸¸è§ `[R]`ã€`[R1]` ç­‰ R-group è®°å·ï¼Œè¿™äº›ä¸æ˜¯æ ‡å‡†åˆ†å­ç¬¦å·ï¼ŒRDKit æ— æ³•è§£æã€‚åŒä¸€ä¸ªåˆ†å­ä¹Ÿå¯èƒ½æœ‰å¤šç§å†™æ³•ï¼ˆå¦‚ `*C=C(*)C` å’Œ `*C(=C*)C`ï¼‰ï¼Œè‹¥ä¸æ ‡å‡†åŒ–å°†å¯¼è‡´å»é‡å¤±æ•ˆã€‚

Polymer SMILES frequently contain R-group tokens like `[R]`, `[R1]` that are non-standard and unparseable by RDKit. The same molecule can also have multiple SMILES representations; without canonicalization, deduplication fails.

```
åŸå§‹ SMILES â†’ æ£€æµ‹å¹¶ç§»é™¤ R-group æ¨¡å¼ â†’ RDKit MolFromSmiles æ ¡éªŒ
Raw SMILES â†’ Detect & remove R-group patterns â†’ RDKit MolFromSmiles validation
         â†“                                              â†“
    è¿”å› Noneï¼ˆä¸¢å¼ƒï¼‰                          MolToSmiles(canonical=True)
    Return None (discard)                      Canonical SMILES output
```

### å¤–éƒ¨æ•°æ®æº / External Data Sources

| æ•°æ®æº / Source | ç›®æ ‡ / Target | è¯´æ˜ / Notes |
|---|---|---|
| `Tg_SMILES_class_pid_polyinfo_median.csv` | Tg | PolyInfo æ±‡ç¼–ï¼Œæ ·æœ¬é‡å¤§ / PolyInfo compilation |
| `JCIM_sup_bigsmiles.csv` | Tg | JCIM è¡¥å……æ•°æ®ï¼Œéœ€åˆ—é‡å‘½å (`Tg (C)` â†’ `Tg`) / Column rename needed |
| `data_tg3.xlsx` | Tg | å•ä½ä¸º Kï¼Œéœ€å‡ 273.15 è½¬æ¢ä¸º Â°C / Unit is K, subtract 273.15 |
| `Tc_SMILES.csv` | Tc | åˆ—é‡å‘½å `TC_mean` â†’ `Tc` / Column rename |
| `data_dnst1.xlsx` | Density | éœ€æ ¡æ­£åç§» (âˆ’0.118)ï¼Œå¹¶è¿‡æ»¤éæ•°å€¼è¡Œ / Offset correction needed |
| `dataset4.csv` (è¡¥å……é›†) | FFV | æ¯”èµ›å®˜æ–¹è¡¥å……æ•°æ® / Official supplementary data |

å¹¶å…¥ç­–ç•¥ï¼šä»¥ canonical SMILES ä¸ºé”®ï¼Œå…ˆå¡«å……è®­ç»ƒé›†ä¸­ç¼ºå¤±çš„ç›®æ ‡å€¼ï¼Œå†è¿½åŠ è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„æ–°åˆ†å­ã€‚åŒä¸€ SMILES å¯¹åº”çš„å¤šä¸ªæ ‡ç­¾å€¼å–å‡å€¼èšåˆã€‚

Integration strategy: Use canonical SMILES as key. First fill missing target values in the training set, then append new molecules not present in training. Multiple labels for the same SMILES are mean-aggregated.

---

## ğŸ”¬ ä¸‰è§†è§’å»ºæ¨¡ / Three-Route Modeling

### è·¯çº¿ Aï¼šåˆ†å­å›¾ GNN / Route A: Molecular Graph GNN

**è¡¨å¾ / Representationï¼š** å°†åˆ†å­è§£æä¸ºèŠ‚ç‚¹-è¾¹å›¾ç»“æ„ã€‚

| å±‚çº§ / Level | ç‰¹å¾ / Features |
|---|---|
| èŠ‚ç‚¹ï¼ˆåŸå­ï¼‰/ Node (Atom) | åŸå­åºæ•°ã€åº¦ã€å½¢å¼ç”µè·ã€æ‚åŒ–æ€ã€èŠ³é¦™æ€§ã€æ€»æ°¢æ•°ã€æ˜¯å¦åœ¨ç¯ã€åŸå­è´¨é‡ (8 ç»´) / Atomic num, degree, formal charge, hybridization, aromaticity, total H, in-ring, mass |
| è¾¹ï¼ˆé”®ï¼‰/ Edge (Bond) | é”®ç±»å‹ï¼ˆfloatï¼‰ã€æ˜¯å¦åœ¨ç¯ã€æ˜¯å¦å…±è½­ã€æ˜¯å¦èŠ³é¦™ (4 ç»´) / Bond type, in-ring, conjugated, aromatic |
| åˆ†å­çº§ / Molecule | MolWtã€HBDã€HBAã€TPSAã€å¯æ—‹è½¬é”®æ•°ã€SMILES é•¿åº¦ (6 ç»´) / Molecular weight, H-bond donors/acceptors, TPSA, rotatable bonds, SMILES length |

**æ¨¡å‹æ¶æ„ / Architectureï¼š**

```mermaid
flowchart TD
    Input["è¾“å…¥å›¾ Input Graph\nèŠ‚ç‚¹ç‰¹å¾ [N, 8] Â· è¾¹ç‰¹å¾ [E, 4]"]

    Input --> GCN["GCN å±‚æ ˆ (Ã—2)\nhidden=128, BatchNorm + ReLU"]
    GCN --> GAT["GAT æ³¨æ„åŠ›å±‚\nheads=4, hidden=128//4"]
    GAT --> Pool["å…¨å±€æ± åŒ– Global Pooling\nmean pool [B, 128] Â· max pool [B, 128]"]
    Pool --> Concat["æ‹¼æ¥ Concat\n[mean Â· max Â· mol_features] â†’ [B, 320]"]
    Concat --> Heads["ç‹¬ç«‹å›å½’å¤´ Ã—5\nEach: Linearâ†’ReLUâ†’Dropoutâ†’Linearâ†’ReLUâ†’Dropoutâ†’Linear(1)"]
    Heads --> Output["5 ç›®æ ‡è¾“å‡º\n5 Target Outputs"]
```

**è®­ç»ƒç­–ç•¥ / Trainingï¼š**
- 5-fold CVï¼Œæ¯ä¸ªç›®æ ‡ä»…åœ¨è¯¥ç›®æ ‡æœ‰æ ‡ç­¾çš„æ ·æœ¬ä¸Šè®­ç»ƒï¼ˆå•ä»»åŠ¡æ¨¡å¼ï¼‰
- ä¼˜åŒ–å™¨ï¼šAdamW (lr=1e-4, weight_decay=5e-4)
- å­¦ä¹ ç‡è°ƒåº¦ï¼šReduceLROnPlateau (patience=10)
- æ—©åœï¼špatience=20ï¼Œæ¢å¤æœ€ä½³æƒé‡
- ç›®æ ‡å€¼æ ‡å‡†åŒ–ï¼šper-fold StandardScalerï¼ˆä»… fit åœ¨è®­ç»ƒæŠ˜ä¸Šï¼‰
- æŸå¤±ï¼šMSELossï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´è®¡ç®—ï¼‰

---

### è·¯çº¿ Bï¼šMordred æè¿°ç¬¦ + CatBoost / Route B: Mordred + CatBoost

**è¡¨å¾ / Representationï¼š** ä½¿ç”¨ Mordred åº“è®¡ç®—å…¨å¥— 2D åˆ†å­æè¿°ç¬¦ï¼ˆçº¦ 1600 ç»´ï¼‰ã€‚é¢„è®¡ç®—çš„è®­ç»ƒé›†æè¿°ç¬¦è¡¨ï¼ˆ`modred-dataset` æ•°æ®é›†ï¼‰ç›´æ¥åŠ è½½ï¼›æµ‹è¯•é›†åœ¨çº¿è®¡ç®—å¹¶ä¸è®­ç»ƒåˆ—å¯¹é½ã€‚

Mordred computes a full suite of 2D molecular descriptors (~1600 dims). Pre-computed training descriptors are loaded from the `modred-dataset`; test descriptors are computed online and column-aligned with training.

**æ•°æ®é¢„å¤„ç† / Preprocessingï¼š**
- ç§»é™¤å¸¸æ•°åˆ—ï¼ˆ`nunique == 1`ï¼‰
- ç§»é™¤ object/category ç±»å‹åˆ—ï¼ˆéæ•°å€¼ï¼‰
- è®­ç»ƒå’Œæµ‹è¯•å–åˆ—äº¤é›†

**æ¨¡å‹ / Modelï¼š** CatBoostRegressorï¼Œæ¯ä¸ªç›®æ ‡ç‹¬ç«‹è®­ç»ƒï¼Œä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼ˆæ—  CVï¼Œç›´æ¥å…¨é›† fit åé¢„æµ‹ testï¼‰ã€‚

---

### è·¯çº¿ Cï¼šæŒ‡çº¹ + å›¾ç»Ÿè®¡ + XGBoost / Route C: Fingerprints + Graph Stats + XGBoost

**è¡¨å¾ / Representationï¼š** ä¸‰ç±»ç‰¹å¾æ‹¼æ¥ã€‚

| ç‰¹å¾ç±»åˆ« / Feature Category | ç»´åº¦ / Dims | è¯´æ˜ / Description |
|---|---|---|
| Morgan æŒ‡çº¹ / Morgan FP | 128 | radius=2, äºŒå…ƒ bit å‘é‡ / Binary bit vector |
| MACCS æŒ‡çº¹ / MACCS FP | 167 | æ ‡å‡† 166 ä½ç»“æ„é”® / Standard 166-bit structural keys |
| RDKit ç‰©åŒ–æè¿°ç¬¦ / RDKit Descriptors | ~200 | MolWtã€LogPã€TPSA ç­‰å…¨å¥— / Full RDKit descriptor set |
| NetworkX å›¾ç»Ÿè®¡ / Graph Statistics | 3 | å›¾ç›´å¾„ã€å¹³å‡æœ€çŸ­è·¯å¾„ã€ç¯æ•° / Graph diameter, avg shortest path, cycle count |

**ç›®æ ‡ç‰¹å¼‚æ€§ç‰¹å¾ç­›é€‰ / Per-Target Feature Selectionï¼š**

æ¯ä¸ªç›®æ ‡ç»´æŠ¤ä¸€ä»½ç‹¬ç«‹çš„ç‰¹å¾ç™½åå•ï¼ˆé€šè¿‡ä¹‹å‰çš„å®éªŒç¡®å®šï¼‰ï¼Œåœ¨ç™½åå•åŸºç¡€ä¸Šå†å åŠ æ–¹å·®é˜ˆå€¼ç­›é€‰ï¼ˆthreshold=0.01ï¼‰ã€‚è¿™æ˜¯è·¯çº¿ C ä¸­é™ä½å™ªå£°ç‰¹å¾å¹²æ‰°çš„å…³é”®æœºåˆ¶ã€‚

Each target maintains an independent feature whitelist (determined by prior experimentation). Variance-threshold pruning (threshold=0.01) is applied on top. This is the key mechanism for reducing noise feature interference in Route C.

**æ•°æ®å¢å¼º / Data Augmentationï¼ˆä¸¤ç§ï¼‰ï¼š**

1. **éšæœº SMILES å¢å¼º**ï¼šå¯¹æ¯ä¸ªè®­ç»ƒåˆ†å­ç”Ÿæˆ 1 ä¸ªéšæœºåŒ– SMILESï¼ˆ`MolToSmiles(doRandom=True)`ï¼‰ï¼Œä¿æŒæ ‡ç­¾ä¸å˜ï¼Œå®ç°è®­ç»ƒé›† 2Ã—ã€‚
   - Random SMILES augmentation: Generate 1 randomized SMILES per molecule, keeping the label unchanged, effectively doubling training data.

2. **GMM åˆæˆå¢å¼º**ï¼šåœ¨ç‰¹å¾+æ ‡ç­¾çš„è”åˆç©ºé—´ä¸­æ‹Ÿåˆé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆ5 componentsï¼‰ï¼Œé‡‡æ · 1000 ä¸ªåˆæˆæ ·æœ¬è¿½åŠ åˆ°è®­ç»ƒé›†ã€‚
   - GMM synthetic augmentation: Fit a Gaussian Mixture Model (5 components) on the joint feature+label space, sample 1000 synthetic points.

**æ¨¡å‹ / Modelï¼š** XGBRegressorï¼Œæ¯ä¸ªç›®æ ‡ç‹¬ç«‹å»ºæ¨¡ï¼Œè¶…å‚æ•°ç»è¿‡ Optuna è°ƒå‚ï¼š

| ç›®æ ‡ / Target | n_estimators | learning_rate | max_depth | reg_lambda |
|---|---|---|---|---|
| Tg | 2173 | 0.067 | 6 | 5.55 |
| FFV | 2202 | 0.072 | 4 | 2.89 |
| Tc | 1488 | 0.010 | 5 | 9.97 |
| Density | 1958 | 0.110 | 5 | 3.07 |
| Rg | 520 | 0.073 | 5 | 0.97 |

---

## âš–ï¸ æ¨¡å‹èåˆ / Model Fusion

æœ€ç»ˆæäº¤ç”±ä¸‰è·¯æ¨¡å‹çš„çº¿æ€§åŠ æƒèåˆç”Ÿæˆï¼Œæƒé‡å¯¹äº”ä¸ªç›®æ ‡ç»Ÿä¸€ï¼š

The final submission is a linear blend of the three routes with uniform weights across all five targets:

```python
w_gnn  = 0.4
w_cat  = 0.3
w_xgb  = 0.3

for col in ['Tg', 'FFV', 'Tc', 'Density', 'Rg']:
    submission[col] = w_gnn * gnn_pred[col] + w_cat * cat_pred[col] + w_xgb * xgb_pred[col]
```

**æƒé‡é€‰æ‹©åŸç† / Weight Rationaleï¼š** GNN ä½œä¸ºå”¯ä¸€èƒ½ç›´æ¥ç¼–ç åˆ†å­æ‹“æ‰‘çš„æ¨¡å‹å æœ€é«˜æƒé‡ï¼›CatBoost å’Œ XGBoost åœ¨é«˜ç»´æè¿°ç¬¦ä¸Šäº’è¡¥ï¼Œå„å  0.3ã€‚ä¸‰è·¯æ¨¡å‹çš„ç‰¹å¾ç©ºé—´å‡ ä¹æ— äº¤å ï¼Œè¿™æ˜¯çº¿æ€§èåˆæœ‰æ•ˆçš„å‰æã€‚

GNN gets the highest weight as the only model that directly encodes molecular topology. CatBoost and XGBoost complement each other on high-dimensional descriptors, each at 0.3. The three routes' feature spaces are nearly non-overlapping, which is the prerequisite for linear fusion to work well.

---

## ğŸ›  å…³é”®å·¥ç¨‹å†³ç­– / Key Engineering Decisions

### 1. ç¦»çº¿ä¾èµ–ç®¡ç† / Offline Dependency Management

Kaggle Notebook ç¦ç½‘ç¯å¢ƒè¦æ±‚æ‰€æœ‰åŒ…ç¦»çº¿å®‰è£…ã€‚å…³é”®ä¾èµ–é€šè¿‡ä¸Šä¼ ä¸º Kaggle Dataset å¹¶ç”¨ `--no-index --find-links` å®‰è£…ï¼š

| ä¾èµ– / Dependency | å®‰è£…æ–¹å¼ / Install Method |
|---|---|
| torch-molecule | `.whl` ç¦»çº¿å®‰è£… / Offline `.whl` |
| torch_geometric 2.6.1 | `.whl` ç¦»çº¿å®‰è£… / Offline `.whl` |
| RDKit 2025.3.3 | `.whl` ç¦»çº¿å®‰è£… / Offline `.whl` |
| Mordred | `--no-index --find-links` / Offline pip |

### 2. GNN å•ä»»åŠ¡ vs å¤šä»»åŠ¡ / GNN Single-task vs Multi-task

ä»£ç ä¸­ `PolymerGNN` æ¶æ„æ”¯æŒå¤šä»»åŠ¡è¾“å‡ºï¼ˆ5 ä¸ªç‹¬ç«‹å›å½’å¤´ï¼‰ï¼Œä½†å®é™…è®­ç»ƒæ—¶æ¯ä¸ªç›®æ ‡ç‹¬ç«‹å¯åŠ¨ä¸€è½® 5-fold CVã€‚åŸå› æ˜¯äº”ä¸ªç›®æ ‡çš„éç©ºæ ·æœ¬é›†åˆå„ä¸ç›¸åŒâ€”â€”å¤šä»»åŠ¡å­¦ä¹ ä¼šåœ¨ç¼ºæ ‡ç­¾ä½ç½®å¼ºåˆ¶å¡«å…¥ 0ï¼Œå¼•å…¥å™ªå£°ã€‚

The `PolymerGNN` architecture supports multi-task output (5 independent heads), but each target is trained in its own 5-fold CV loop. The reason: the non-null sample sets differ per target â€” multi-task learning would force 0-fill at missing positions, introducing noise.

### 3. ç›®æ ‡å€¼æ ‡å‡†åŒ–ä½ç½® / Target Normalization Placement

GNN è·¯çº¿ä¸­ï¼Œç›®æ ‡å€¼åœ¨æ¯ä¸€æŠ˜å†…ç”¨ StandardScaler æ ‡å‡†åŒ–ï¼ˆä»… fit åœ¨è®­ç»ƒæŠ˜ï¼‰ï¼Œé¢„æµ‹ååæ ‡å‡†åŒ–å›åŸå§‹ç©ºé—´ã€‚è¿™é¿å…äº†ä¸åŒç›®æ ‡å°ºåº¦ä¹‹é—´çš„å¹²æ‰°ï¼Œå¹¶ä¿è¯äº†è®­ç»ƒ/éªŒè¯çš„ä¸¥æ ¼åˆ†ç¦»ã€‚

In the GNN route, target values are standardized per-fold with StandardScaler (fit on training fold only), then inverse-transformed after prediction. This prevents scale interference across targets and ensures strict train/val separation.

### 4. æµ‹è¯•é›† Mordred æè¿°ç¬¦çš„åœ¨çº¿è®¡ç®— / Online Mordred Computation for Test

è®­ç»ƒé›†çš„ Mordred æè¿°ç¬¦ä½¿ç”¨é¢„è®¡ç®—è¡¨ï¼ˆé¿å…é«˜æ˜‚çš„è®¡ç®—ä»£ä»·ï¼‰ï¼Œä½†æµ‹è¯•é›†å¿…é¡»åœ¨çº¿è®¡ç®—ã€‚è®¡ç®—åä¸è®­ç»ƒåˆ—å–äº¤é›†å¯¹é½ï¼Œé¿å…ç‰¹å¾ä¸åŒ¹é…ã€‚

Training Mordred descriptors use a pre-computed table (avoiding expensive computation), but test descriptors must be computed online. After computation, columns are intersected with training columns to avoid feature mismatch.

---

## ğŸ“ ä»“åº“ç»“æ„ / Repository Structure

```
Open-Polymer-Prediction-2025/
â”‚
â”œâ”€â”€ README.md                      â† æœ¬æ–‡ä»¶ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰/ This file (bilingual)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ gnn-train.ipynb            â† GNN è®­ç»ƒ + å¤–éƒ¨æ•°æ®å¹¶å…¥ + é¢„æµ‹ç”Ÿæˆ gnn_sub.csv
â”‚   â”‚                                 GNN training + external data integration + gnn_sub.csv
â”‚   â””â”€â”€ inference.ipynb            â† CatBoost/XGBoost æ¨ç† + ä¸‰è·¯èåˆ â†’ submission.csv
â”‚                                     CatBoost/XGBoost inference + three-route fusion â†’ submission.csv
â”‚
â””â”€â”€ outputs/                       â† ç”Ÿæˆäº§ç‰© (gitignored) / Generated outputs
    â”œâ”€â”€ gnn_sub.csv                â† GNN è·¯çº¿é¢„æµ‹ / GNN route predictions
    â”œâ”€â”€ cat_sub.csv                â† CatBoost è·¯çº¿é¢„æµ‹ / CatBoost route predictions
    â”œâ”€â”€ xgb_sub.csv                â† XGBoost è·¯çº¿é¢„æµ‹ / XGBoost route predictions
    â””â”€â”€ submission.csv             â† æœ€ç»ˆèåˆæäº¤ / Final fused submission
```

### Notebook æ‰§è¡Œé¡ºåº / Notebook Execution Order

```
1. gnn-train.ipynb
   â”œâ”€â”€ ç¦»çº¿å®‰è£…ä¾èµ– (torch-molecule, PyG, RDKit, Mordred)
   â”œâ”€â”€ åŠ è½½ + æ¸…ç†è®­ç»ƒ/æµ‹è¯• SMILES
   â”œâ”€â”€ å¹¶å…¥å¤–éƒ¨æ•°æ® â†’ train_extended
   â”œâ”€â”€ 5-fold GNN è®­ç»ƒï¼ˆæ¯ç›®æ ‡ç‹¬ç«‹ï¼‰
   â””â”€â”€ è¾“å‡º gnn_sub.csv + ä¿å­˜æ¨¡å‹æƒé‡ (NN_{target}_fold_{fold}_best.pth)

2. inference.ipynb
   â”œâ”€â”€ ç¦»çº¿å®‰è£…ä¾èµ–ï¼ˆåŒä¸Šï¼‰
   â”œâ”€â”€ [cat_infer.py] Mordred æè¿°ç¬¦è®¡ç®— â†’ CatBoost æ¨ç† â†’ cat_sub.csv
   â”œâ”€â”€ [gnn_infer.py] åŠ è½½å·²è®­ç»ƒ GNN æƒé‡ â†’ æ¨ç† â†’ gnn_sub.csv
   â”œâ”€â”€ [xgb_infer.py] æŒ‡çº¹+å›¾ç»Ÿè®¡+å¢å¼º â†’ XGBoost æ¨ç† â†’ xgb_sub.csv
   â””â”€â”€ ä¸‰è·¯åŠ æƒèåˆ â†’ submission.csv
```

---

## ğŸš€ å¤ç°æŒ‡å— / Reproduction Guide

### ç¯å¢ƒè¦æ±‚ / Environment Requirements

| é¡¹ç›® / Item | è¦æ±‚ / Requirement |
|---|---|
| **å¹³å° / Platform** | Kaggle Notebookï¼ˆGPU enabledï¼‰ |
| **Python** | 3.11 |
| **GPU** | Kaggle æä¾›çš„ P100 / T4 |
| **æ—¶é—´é™åˆ¶ / Time Limit** | â‰¤ 9 å°æ—¶ï¼ˆæ€»è®­ç»ƒ+æ¨ç†ï¼‰/ â‰¤ 9 hours total |

### æ ¸å¿ƒä¾èµ– / Key Dependencies

```
torch >= 2.0
torch_geometric == 2.6.1
rdkit >= 2025.3.3
mordred
catboost
xgboost
lightgbm
scikit-learn
pandas, numpy, networkx
```

### æ“ä½œæ­¥éª¤ / Step-by-Step

```
1. åœ¨ Kaggle æ–°å»º Notebookï¼Œæ·»åŠ ä»¥ä¸‹æ•°æ®é›†ï¼š
   Create a new Kaggle Notebook and add the following datasets:
     - neurips-open-polymer-prediction-2025ï¼ˆæ¯”èµ›æ•°æ®ï¼‰
     - modred-datasetï¼ˆé¢„è®¡ç®— Mordred æè¿°ç¬¦ï¼‰
     - tg-smiles-pid-polyinfo-classï¼ˆå¤–éƒ¨ Tg æ•°æ®ï¼‰
     - smiles-extra-dataï¼ˆå¤–éƒ¨ Tg/Density æ•°æ®ï¼‰
     - tc-smilesï¼ˆå¤–éƒ¨ Tc æ•°æ®ï¼‰
     - torch-molecule-whl / torch-geometric-2-6-1 / rdkitï¼ˆç¦»çº¿åŒ…ï¼‰
     - mordred-1-2-0-py3-none-anyï¼ˆç¦»çº¿åŒ…ï¼‰

2. è¿è¡Œ gnn-train.ipynb å…¨éƒ¨ Cell
   Run all cells in gnn-train.ipynb
     â†’ è¾“å‡ºï¼šgnn_sub.csv + æ¨¡å‹æƒé‡æ–‡ä»¶
     â†’ Output: gnn_sub.csv + model weight files

3. åœ¨æ–° Notebook ä¸­è¿è¡Œ inference.ipynb
   Run inference.ipynb in a new Notebook (æ·»åŠ  gnn-bert-hvec-concat-nn æ•°æ®é›†æŒ‡å‘æƒé‡)
     â†’ ä¾æ¬¡æ‰§è¡Œ cat_infer.py â†’ gnn_infer.py â†’ xgb_infer.py â†’ èåˆ
     â†’ Sequentially: cat_infer â†’ gnn_infer â†’ xgb_infer â†’ fusion
     â†’ è¾“å‡ºï¼šsubmission.csv
     â†’ Output: submission.csv

4. æäº¤ submission.csv åˆ°æ¯”èµ›é¡µ
   Submit submission.csv to the competition page
```

---

## ğŸ“ åæ€ä¸æœªæ¥æ–¹å‘ / Reflections & Future Directions

### æ–¹æ³•è®ºåæ€ / Methodological Reflections

**1. æ•°æ®æ²»ç†çš„æƒé‡è¢«ä½ä¼° / Data curation weight is underestimated**

åœ¨è¿™åœºæ¯”èµ›ä¸­ï¼Œæ•°æ®å±‚é¢çš„å·¥ä½œâ€”â€”R-group è¿‡æ»¤ã€canonical åŒ–ã€å¤–éƒ¨æ•°æ®å¹¶å…¥ä¸å»é‡â€”â€”å¯¹æœ€ç»ˆæˆç»©çš„è´¡çŒ®å¯èƒ½è¶…è¿‡ä»»ä½•å•ä¸ªæ¨¡å‹æ¶æ„çš„ä¼˜åŒ–ã€‚è„æ•°æ®é™é»˜ä¼ æ’­åˆ°æ¨¡å‹è®­ç»ƒï¼Œå¾€å¾€ä¸ä¼šæŠ¥é”™ï¼Œä½†ä¼šå¯¼è‡´é¢„æµ‹åç§»éš¾ä»¥æ’æŸ¥ã€‚

In this competition, data-layer work â€” R-group filtering, canonicalization, external data integration and deduplication â€” likely contributed more to final performance than any single model architecture optimization. Dirty data propagates silently into model training, rarely causing explicit errors but leading to hard-to-diagnose prediction drift.

**2. ä¸‰è·¯èåˆçš„å¤šæ ·æ€§æ¥æº / Diversity source of three-route fusion**

èåˆæœ‰æ•ˆçš„æ ¸å¿ƒå‰ææ˜¯æ¨¡å‹é—´çš„é¢„æµ‹å¤šæ ·æ€§ã€‚æœ¬æ–¹æ¡ˆä¸­ä¸‰è·¯æ¨¡å‹çš„ç‰¹å¾ç©ºé—´å‡ ä¹ä¸é‡å ï¼ˆå›¾ç»“æ„ vs é«˜ç»´æè¿°ç¬¦ vs æŒ‡çº¹ï¼‰ï¼Œè¿™å¤©ç„¶ä¿è¯äº†å¤šæ ·æ€§ã€‚è‹¥ä¸‰è·¯éƒ½ç”¨ç›¸ä¼¼çš„ç‰¹å¾ï¼Œèåˆæ”¶ç›Šä¼šå¤§å¹…ç¼©å‡ã€‚

The core prerequisite for effective fusion is prediction diversity across models. In this solution, the three routes' feature spaces are nearly non-overlapping (graph structure vs. high-dim descriptors vs. fingerprints), naturally guaranteeing diversity. If all three used similar features, fusion gains would shrink dramatically.

**3. ç›®æ ‡ç‹¬ç«‹å»ºæ¨¡çš„å–èˆ / Trade-off of per-target modeling**

ç‹¬ç«‹å»ºæ¨¡è®©æ¯ä¸ªç›®æ ‡å¯ä»¥é€‰æ‹©æœ€ä¼˜è®­ç»ƒå­é›†å’Œè¶…å‚æ•°ï¼Œä½†ä¹Ÿæ”¾å¼ƒäº†ç›®æ ‡ä¹‹é—´æ½œåœ¨çš„å…±äº«è¡¨å¾ã€‚å¯¹äºæœ¬èµ›äº‹è¿™ç§æ•°æ®é‡å·®å¼‚æå¤§çš„åœºæ™¯ï¼Œç‹¬ç«‹å»ºæ¨¡æ˜¯åˆç†é€‰æ‹©ï¼›è‹¥æ•°æ®æ›´å……è¶³ï¼Œå¤šä»»åŠ¡å­¦ä¹ å¯èƒ½å€¼å¾—é‡æ–°æ¢ç´¢ã€‚

Independent modeling lets each target select its optimal training subset and hyperparameters, but gives up potential shared representations across targets. For this competition's extreme sample-size heterogeneity, independent modeling is reasonable; with more data, multi-task learning might be worth revisiting.

### æ˜ç¡®ä¸è¶³ / Known Limitations

| ä¸è¶³ / Limitation | è¯´æ˜ / Description |
|---|---|
| **èåˆæƒé‡æœªä¼˜åŒ–** / Fusion weights not optimized | æƒé‡ (0.4/0.3/0.3) ä¸ºæ‰‹åŠ¨è®¾å®šï¼Œæœªé€šè¿‡ OOF é¢„æµ‹ä¼˜åŒ– / Weights are manually set, not optimized via OOF predictions |
| **CatBoost æ— äº¤å‰éªŒè¯** / CatBoost without CV | CatBoost è·¯çº¿ç›´æ¥å…¨é›† fitï¼Œæ—  OOF é¢„æµ‹ï¼Œæ³›åŒ–æ€§ä¼°è®¡ä¸å¯é  / Full-set fit with no OOF; generalization estimate unreliable |
| **GMM å¢å¼ºæ•ˆæœæœªæ¶ˆè** / GMM augmentation not ablated | åˆæˆæ ·æœ¬çš„è´¡çŒ®æœªå•ç‹¬éªŒè¯ï¼Œå¯èƒ½æ˜¯å™ªå£° / Synthetic sample contribution not verified; could be noise |
| **æœªåˆ©ç”¨ 3D ä¿¡æ¯** / No 3D information used | ä»…ä½¿ç”¨ 2D ç»“æ„ï¼Œæœªå°è¯• 3D æ„è±¡ç”Ÿæˆæˆ–èƒ½é‡æœ€å°åŒ– / Only 2D structure used; no 3D conformation or energy minimization attempted |

### ä¸‹ä¸€æ­¥æ–¹å‘ / Future Directions

1. **OOF ä¼˜åŒ–èåˆæƒé‡ / OOF-based fusion weight optimization** â€” ç”¨å…¨éƒ¨æ¨¡å‹çš„ OOF é¢„æµ‹å¯¹ wMAE åšæ¢¯åº¦ä¼˜åŒ–ï¼Œè€Œéæ‰‹åŠ¨è®¾å®šæƒé‡ã€‚
2. **å¤šä»»åŠ¡ GNN + ç¼ºå€¼æ©ç  / Multi-task GNN with missing-value masking** â€” å¯¹ç¼ºæ ‡ç­¾ä½ç½®ä½¿ç”¨æ©ç æŸå¤±ï¼Œé¿å…å¡«å…¥ 0 å™ªå£°ï¼ŒåŒæ—¶äº«å—å…±äº«è¡¨å¾çš„å¥½å¤„ã€‚
3. **Stacking è€Œéç®€å•æ··åˆ / Stacking instead of simple blending** â€” ç”¨ä¸‰è·¯ OOF é¢„æµ‹ä½œä¸ºå…ƒç‰¹å¾ï¼Œè®­ç»ƒä¸€å±‚å…ƒå­¦ä¹ å™¨ã€‚
4. **3D åˆ†å­è¡¨å¾ / 3D molecular representation** â€” ç”Ÿæˆ 3D æ„è±¡å¹¶ä½¿ç”¨ SchNet/DimeNet ç­‰ 3D-aware GNNï¼Œæ•æ‰ç©ºé—´æ„è±¡ä¿¡æ¯ã€‚
5. **æŒ‰ç›®æ ‡ä¼˜åŒ–èåˆ / Per-target fusion optimization** â€” ä¸åŒç›®æ ‡å¯èƒ½åœ¨ä¸åŒè·¯çº¿ä¸Šä¼˜åŠ¿ä¸åŒï¼Œåº”ç‹¬ç«‹ä¼˜åŒ–æ¯ä¸ªç›®æ ‡çš„èåˆæƒé‡ã€‚

---

## ğŸ›  æŠ€æœ¯æ ˆ / Tech Stack

| ç±»åˆ« / Category | å·¥å…· / Tools |
|---|---|
| æ·±åº¦å­¦ä¹  / Deep Learning | PyTorch 2.x, PyTorch Geometric 2.6.1 (GCNConv, GATConv) |
| æ¢¯åº¦æå‡ / Gradient Boosting | CatBoost, XGBoost |
| åˆ†å­å¤„ç† / Cheminformatics | RDKit 2025.3.3, Mordred |
| å›¾åˆ†æ / Graph Analysis | NetworkX |
| æœºå™¨å­¦ä¹ å·¥å…· / ML Utilities | scikit-learn (KFold, StandardScaler, VarianceThreshold) |
| æ•°æ®å¤„ç† / Data | Pandas, NumPy |
| è¿è¡Œç¯å¢ƒ / Environment | Kaggle Notebooks (P100 GPU, 9h limit) |

---
